import os
import shutil
import re
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from src.models import get_embedding_model


DATA_PATH = "./data"
DB_PATH = "./chroma_db"
MAX_CHUNK_SIZE = 1500  # K√≠ch th∆∞·ªõc t·ªëi ƒëa c·ªßa 1 chunk (k√Ω t·ª±)

def create_vector_db():
    print("B·∫ÆT ƒê·∫¶U T·∫†O VECTOR DATABASE")

    # D·ªçn d·∫πp DB c≈©
    if os.path.exists(DB_PATH):
        shutil.rmtree(DB_PATH)
        print(f"ƒê√£ x√≥a database c≈© t·∫°i {DB_PATH}")

    # Load t√†i li·ªáu
    if not os.path.exists(DATA_PATH):
        print(f"Th∆∞ m·ª•c {DATA_PATH} kh√¥ng t·ªìn t·∫°i!")
        return

    loader = DirectoryLoader(DATA_PATH, glob="**/*.txt", loader_cls=TextLoader, loader_kwargs={"encoding": "utf-8"})
    raw_documents = loader.load()
    print(f"ƒê√£ t·∫£i {len(raw_documents)} file t√†i li·ªáu")

    # X·ª≠ l√Ω & Chunking
    all_chunks = []
    
    for doc in raw_documents:
        file_name = os.path.basename(doc.metadata.get("source", ""))
        content = doc.page_content
        
        # Ph√¢n lo·∫°i t√†i li·ªáu ƒë·ªÉ √°p d·ª•ng chi·∫øn thu·∫≠t c·∫Øt
        if is_legal_document(file_name):
            print(f"X·ª≠ l√Ω Quy ch·∫ø: {file_name}")
            chunks = split_legal_document(content, doc.metadata)
        else:
            print(f"X·ª≠ l√Ω S·ªï tay/Markdown: {file_name}")
            chunks = split_markdown_document(content, doc.metadata)
            
        all_chunks.extend(chunks)

    print(f"T·ªïng s·ªë chunk t·∫°o ra: {len(all_chunks)}")
    
    # In 3 chunk ƒë·∫ßu
    print_debug_chunks(all_chunks)

    # L∆∞u v√†o ChromaDB
    print("\nƒêang m√£ h√≥a (Embedding) v√† l∆∞u v√†o DB...")
    embedding_model = get_embedding_model()
    vector_db = Chroma.from_documents(
        documents=all_chunks,
        embedding=embedding_model,
        persist_directory=DB_PATH
    )
    vector_db.persist()
    print(f"HO√ÄN T·∫§T! Database s·∫µn s√†ng t·∫°i: {DB_PATH}")



def is_legal_document(filename):
    """Nh·∫≠n di·ªán file quy ch·∫ø d·ª±a tr√™n t√™n file"""
    keywords = ["quyche", "quydinh", "quyetdinh", "luat", "daotao", "hocphi"]
    return any(k in filename.lower() for k in keywords)

def split_legal_document(text, metadata):
    """
    Chi·∫øn thu·∫≠t cho Quy ch·∫ø:
    1. T√°ch theo Ch∆∞∆°ng (ƒë·ªÉ l·∫•y ng·ªØ c·∫£nh l·ªõn).
    2. Trong Ch∆∞∆°ng, t√°ch theo ƒêi·ªÅu.
    3. Trong ƒêi·ªÅu, n·∫øu d√†i qu√° th√¨ t√°ch theo Kho·∫£n (1., 2.) ho·∫∑c √Ω nh·ªè.
    QUAN TR·ªåNG: Lu√¥n g·∫Øn 'ƒêi·ªÅu X...' v√†o ƒë·∫ßu m·ªói chunk con.
    """
    chunks = []
    
    # T√°ch c√°c Ch∆∞∆°ng
    # Regex: T√¨m chu·ªói "CH∆Ø∆†NG [S·ªë La M√£]"
    chapter_splits = re.split(r"(^CH∆Ø∆†NG\s+[IVXLCDM]+.*$)", text, flags=re.MULTILINE)
    
    current_chapter = "Quy ƒë·ªãnh chung"
    
    for i in range(1, len(chapter_splits), 2):
        if i+1 < len(chapter_splits):
            header = chapter_splits[i].strip() # T√™n ch∆∞∆°ng
            body = chapter_splits[i+1]         # N·ªôi dung ch∆∞∆°ng
            
            # Trong m·ªói ch∆∞∆°ng, t√°ch c√°c ƒêi·ªÅu
            # Regex: T√¨m "ƒêi·ªÅu [S·ªë]."
            article_splits = re.split(r"(^ƒêi·ªÅu\s+\d+[\.:]?\s+.*$)", body, flags=re.MULTILINE)
            
            current_article_header = ""
            
            # X·ª≠ l√Ω ph·∫ßn d·∫´n nh·∫≠p c·ªßa ch∆∞∆°ng (n·∫øu c√≥)
            if article_splits[0].strip():
                 chunks.append(create_doc(
                     text=article_splits[0], 
                     meta=metadata, 
                     context=f"{header}"
                 ))

            for k in range(1, len(article_splits), 2):
                if k+1 < len(article_splits):
                    art_header = article_splits[k].strip() # VD: "ƒêi·ªÅu 5. ƒêi·ªÉm h·ªçc ph·∫ßn"
                    art_body = article_splits[k+1]         # N·ªôi dung ƒëi·ªÅu
                    
                    full_context_header = f"{header} > {art_header}"
                    
                    # Ki·ªÉm tra ƒë·ªô d√†i ƒêi·ªÅu
                    full_text = f"{art_header}\n{art_body}"
                    
                    if len(full_text) < MAX_CHUNK_SIZE:
                        # N·∫øu ng·∫Øn, gi·ªØ nguy√™n c·∫£ ƒëi·ªÅu
                        chunks.append(create_doc(art_body, metadata, full_context_header))
                    else:
                        # N·∫øu d√†i, c·∫Øt nh·ªè nh∆∞ng LU√îN K√àM TI√äU ƒê·ªÄ ƒêI·ªÄU
                        sub_chunks = recursive_split(art_body, chunk_size=1000)
                        for sub in sub_chunks:
                            # Context Injection: G·∫Øn ti√™u ƒë·ªÅ v√†o n·ªôi dung ƒë·ªÉ AI hi·ªÉu
                            chunks.append(create_doc(sub, metadata, full_context_header))
                            
    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p vƒÉn b·∫£n kh√¥ng c√≥ Ch∆∞∆°ng, ch·ªâ c√≥ ƒêi·ªÅu
    if not chunks: 
        # Fallback: T√°ch th·∫≥ng theo ƒêi·ªÅu
        article_splits = re.split(r"(^ƒêi·ªÅu\s+\d+[\.:]?\s+.*$)", text, flags=re.MULTILINE)
        for k in range(1, len(article_splits), 2):
            header = article_splits[k].strip()
            body = article_splits[k+1]
            chunks.append(create_doc(body, metadata, header))
            
    return chunks

def split_markdown_document(text, metadata):
    """
    Chi·∫øn thu·∫≠t cho S·ªï tay (Markdown):
    C·∫Øt theo c·∫•p ƒë·ªô Header: # -> ## -> ###
    """
    from langchain_text_splitters import MarkdownHeaderTextSplitter
    
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
    
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    md_docs = markdown_splitter.split_text(text)
    
    final_chunks = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=MAX_CHUNK_SIZE, chunk_overlap=200)
    
    for doc in md_docs:
        # T·∫°o context string t·ª´ metadata header
        context_parts = []
        if "Header 1" in doc.metadata: context_parts.append(doc.metadata["Header 1"])
        if "Header 2" in doc.metadata: context_parts.append(doc.metadata["Header 2"])
        if "Header 3" in doc.metadata: context_parts.append(doc.metadata["Header 3"])
        
        context_str = " > ".join(context_parts)
        
        # N·∫øu chunk qu√° d√†i, c·∫Øt nh·ªè th√†nh c√°c ph·∫ßn
        if len(doc.page_content) > MAX_CHUNK_SIZE:
            splits = text_splitter.split_text(doc.page_content)
            for s in splits:
                final_chunks.append(create_doc(s, metadata, context_str))
        else:
            final_chunks.append(create_doc(doc.page_content, metadata, context_str))
            
    return final_chunks

def recursive_split(text, chunk_size):
    """H√†m c·∫Øt nh·ªè b·ªï tr·ª£"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, 
        chunk_overlap=150,
        separators=["\n\n", "\n", ".", " ", ""]
    )
    return splitter.split_text(text)

def create_doc(text, meta, context):
    """
    T·∫°o Document chu·∫©n h√≥a.
    QUAN TR·ªåNG: G·ªôp Context v√†o page_content ƒë·ªÉ Embedding hi·ªÉu ng·ªØ c·∫£nh.
    """
    # L√†m s·∫°ch text
    text = re.sub(r'\n+', '\n', text).strip()
    
    # N·ªôi dung th·ª±c t·∫ø ƒë∆∞a v√†o Vector DB = [Ti√™u ƒë·ªÅ] + [N·ªôi dung]
    # V√≠ d·ª•: "ƒêi·ªÅu 5. H·ªçc ph√≠... [N·ªôi dung chi ti·∫øt]"
    content_with_context = f"[{context}]\n{text}"
    
    new_meta = meta.copy()
    new_meta["section"] = context # L∆∞u ti√™u ƒë·ªÅ ƒë·ªÉ hi·ªÉn th·ªã ngu·ªìn sau n√†y
    
    return Document(page_content=content_with_context, metadata=new_meta)

def print_debug_chunks(chunks):
    print("\nüîç --- In 3 chunk ƒë·∫ßu ti√™n ---")
    for i, c in enumerate(chunks[:3]):
        print(f"Chunk {i+1}:")
        print(f"   üìÇ File: {c.metadata.get('source')}")
        print(f"   üè∑Ô∏è  Section: {c.metadata.get('section')}")
        print(f"   üìù Content (Preview): {c.page_content[:150]}...")
        print("-" * 50)

if __name__ == "__main__":
    create_vector_db()